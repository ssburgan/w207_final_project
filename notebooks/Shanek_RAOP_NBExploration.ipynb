{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Shanek_RAOP-NBExplortion.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGxXV_paZVg5"
      },
      "outputs": [],
      "source": [
        "### Shane Kramer\n",
        "### 207 Final Project - Exploring Naive Bayes Models\n",
        "### 03.27.22 - Naive Bayes model with weighting scoring, upscaling, downscaling"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas import json_normalize\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import *\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "import re\n",
        "import nltk\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import json \n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "yVjG9sdYbnEg"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SLYTbwwgbo3X",
        "outputId": "8ae179ba-43ca-4e02-a00d-34551f74ffcd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Can configure the list below to filter columns from original training data set\n",
        "test_cols = ['request_text', 'request_number_of_comments_at_retrieval', 'requester_received_pizza']\n",
        "\n",
        "# Read JSON file\n",
        "df = pd.read_json('/content/drive/MyDrive/W207/FinalProject/w207_final_project/src/data/train.json')\n",
        "\n",
        "# For the entire training set, add column for request text count, create label obejct, \n",
        "# And purge the request_texct_count column from training data\n",
        "full_df = df.copy()\n",
        "full_df['request_text_count'] = df['request_text'].str.count(' ') + 1\n",
        "full_labels = full_df['requester_received_pizza']\n",
        "full_df = full_df.drop('requester_received_pizza', axis=1)\n",
        "\n",
        "# Not filtering columns for now\n",
        "#filtered_df = df[test_cols]\n",
        "\n",
        "dev_size=1000\n",
        "\n",
        "# Build train and dev data sets\n",
        "train_data, train_labels = full_df.request_text[dev_size:], full_labels[dev_size:]\n",
        "dev_data, dev_labels = full_df.request_text[:dev_size], full_labels[:dev_size]"
      ],
      "metadata": {
        "id": "GhyNReL4cMod"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"###################################################################\")\n",
        "print(\"Tuning Naive Bayes Alpha for baseline model (score weighted).\")\n",
        "print(\"###################################################################\")\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "trainVector = vectorizer.fit_transform(train_data)\n",
        "devVector = vectorizer.transform(dev_data)\n",
        "\n",
        "for i in [.0001, .001, .01, .05, .06, .07, .08, .09, .1, .11, .5, 1.0, 2.0]:\n",
        "      \n",
        "  mnb = MultinomialNB(alpha = i)\n",
        "  mnb.fit(trainVector, train_labels)\n",
        "\n",
        "  #Evaluate performance on the dev set.\n",
        "  pred_mnb = mnb.predict(devVector)\n",
        "  score = metrics.f1_score(dev_labels, pred_mnb, average=\"weighted\")\n",
        "  print(\"     a =\", i, \" - f1 score =\", score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TImgbVmLc1mE",
        "outputId": "ffb6fd7b-6a8d-44e8-931f-81b63a536d74"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "###################################################################\n",
            "Tuning Naive Bayes Alpha for baseline model (score weighted).\n",
            "###################################################################\n",
            "     a = 0.0001  - f1 score = 0.668475\n",
            "     a = 0.001  - f1 score = 0.6677606796018085\n",
            "     a = 0.01  - f1 score = 0.6708568829248878\n",
            "     a = 0.05  - f1 score = 0.667934345758552\n",
            "     a = 0.06  - f1 score = 0.6701334560515416\n",
            "     a = 0.07  - f1 score = 0.6730585091260589\n",
            "     a = 0.08  - f1 score = 0.6723325663445313\n",
            "     a = 0.09  - f1 score = 0.6700974626398354\n",
            "     a = 0.1  - f1 score = 0.6715402298850575\n",
            "     a = 0.11  - f1 score = 0.6715402298850575\n",
            "     a = 0.5  - f1 score = 0.6568691782147799\n",
            "     a = 1.0  - f1 score = 0.6395443925233647\n",
            "     a = 2.0  - f1 score = 0.6292905273564535\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It looks like the optimal alpha for our model is .07 with an f1 score of 0.67306.\n",
        "\n",
        "Lets preprocess the data first and see what we can accomplish. "
      ],
      "metadata": {
        "id": "NA6IM6eerdxE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\")\n",
        "print(\"###################################################################\")\n",
        "print(\"Let's see if pre-processing the data helps at all.\")\n",
        "print(\"###################################################################\")\n",
        "\n",
        "nltk.download('stopwords')\n",
        "    \n",
        "cachedStopWords = stopwords.words(\"english\")\n",
        "ps = PorterStemmer()\n",
        "\n",
        "def myPreprocessor(data):\n",
        "   filteredData = re.sub('[^A-Za-z0-9 \\n]+', ' ', data.lower())\n",
        "   filteredData = ' '.join([word for word in filteredData.split() \n",
        "      if word not in cachedStopWords])\n",
        "   filteredData = ' '.join([word[0:6] if len(word) > 6 else \n",
        "                            word for word in filteredData.split()])\n",
        "   #print(filteredData)\n",
        "   return filteredData\n",
        "\n",
        "ppVectorizer = CountVectorizer(preprocessor=myPreprocessor)\n",
        "ppTrainVector = ppVectorizer.fit_transform(train_data)\n",
        "ppDevVector = ppVectorizer.transform(dev_data)\n",
        "\n",
        "mnb = MultinomialNB(alpha = .07)\n",
        "mnb.fit(ppTrainVector, train_labels)\n",
        "\n",
        "#Evaluate performance on the dev set.\n",
        "pred_mnb = mnb.predict(ppDevVector)\n",
        "score = metrics.f1_score(dev_labels, pred_mnb, average=\"weighted\")\n",
        "print(\"     a = .07 - f1 score =\", score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEzIw9ddrwT1",
        "outputId": "cc882421-91bd-4e8e-a603-450870d32c67"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "###################################################################\n",
            "Let's see if pre-processing the data helps at all.\n",
            "###################################################################\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "     a = .07 - f1 score = 0.6599911685892124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Maybe TF-IDF?"
      ],
      "metadata": {
        "id": "Sm9z04N3wGjE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\")\n",
        "print(\"###################################################################\")\n",
        "print(\"Give TF-IDF a whirl ... \")\n",
        "print(\"###################################################################\")\n",
        "tfidfVectorizer = TfidfVectorizer()\n",
        "tfidTrainVector = tfidfVectorizer.fit_transform(train_data)\n",
        "tfidDevVector = tfidfVectorizer.transform(dev_data)\n",
        "\n",
        "for i in [.0001, .001, .01, .05, .06, .07, .08, .09, .1, .11, .5, 1.0, 2.0]:\n",
        "  mnb = MultinomialNB(alpha = i)\n",
        "  mnb.fit(tfidTrainVector, train_labels)\n",
        "\n",
        "  #Evaluate performance on the dev set.\n",
        "  pred_mnb = mnb.predict(tfidDevVector)\n",
        "  score = metrics.f1_score(dev_labels, pred_mnb, average=\"weighted\")\n",
        "  print(\"     a =\", i, \" - f1 score =\", score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5XBVrrWwDgF",
        "outputId": "007f5cb8-8e8c-4b11-80ec-cb02697d168d"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "###################################################################\n",
            "Give TF-IDF a whirl ... \n",
            "###################################################################\n",
            "     a = 0.0001  - f1 score = 0.6478824998691284\n",
            "     a = 0.001  - f1 score = 0.6510131562423268\n",
            "     a = 0.01  - f1 score = 0.6461060079443892\n",
            "     a = 0.05  - f1 score = 0.6390767917528482\n",
            "     a = 0.06  - f1 score = 0.6363774499696974\n",
            "     a = 0.07  - f1 score = 0.6341841091586662\n",
            "     a = 0.08  - f1 score = 0.6357733860342556\n",
            "     a = 0.09  - f1 score = 0.6310861397400986\n",
            "     a = 0.1  - f1 score = 0.6310861397400986\n",
            "     a = 0.11  - f1 score = 0.6287911893168738\n",
            "     a = 0.5  - f1 score = 0.6294252873563219\n",
            "     a = 1.0  - f1 score = 0.6294252873563219\n",
            "     a = 2.0  - f1 score = 0.6294252873563219\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\")\n",
        "print(\"###################################################################\")\n",
        "print(\"TF-IDF with pre-processing ... \")\n",
        "print(\"###################################################################\")\n",
        "tfidfVectorizer = TfidfVectorizer(preprocessor=myPreprocessor)\n",
        "tfidTrainVector = tfidfVectorizer.fit_transform(train_data)\n",
        "tfidDevVector = tfidfVectorizer.transform(dev_data)\n",
        "\n",
        "mnb = MultinomialNB(alpha = .001)\n",
        "mnb.fit(tfidTrainVector, train_labels)\n",
        "\n",
        "#Evaluate performance on the dev set.\n",
        "pred_mnb = mnb.predict(tfidDevVector)\n",
        "score = metrics.f1_score(dev_labels, pred_mnb, average=\"weighted\")\n",
        "print(\"     a = .07 - f1 score =\", score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scqT-bZbw416",
        "outputId": "c6bc3d9b-b127-4951-a487-50e8798f3bb5"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "###################################################################\n",
            "TF-IDF with prec-processing ... \n",
            "###################################################################\n",
            "     a = .07 - f1 score = 0.6498128199589787\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It looks like the best I could do is by training on 75% (ish) of the data, tested against 25% (about 3000 and 1000 respectively), running basic multinomial NB model against data that isn't preprocessed, and using an alpha of .07. This achieves an F1 score of 0.67306."
      ],
      "metadata": {
        "id": "ILxbmwvOx6kw"
      }
    }
  ]
}