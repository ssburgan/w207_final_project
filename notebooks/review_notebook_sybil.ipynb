{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af915d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This tells matplotlib not to try opening a new window for each plot.\n",
    "%matplotlib inline\n",
    "\n",
    "# General libraries.\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# SK-learn libraries for learning.\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# SK-learn library for importing the newsgroup data.\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *\n",
    "\n",
    "import nltk\n",
    "\n",
    "import json \n",
    "import pandas as pd \n",
    "from pandas import json_normalize\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4489fb33",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfba5b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load training data\n",
    "with open('../data/random-acts-of-pizza/train.json') as f:\n",
    "    train_json = json.load(f)\n",
    "    \n",
    "# Load test data\n",
    "with open('../data/random-acts-of-pizza/test.json') as f:\n",
    "    test_json = json.load(f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf52d9d",
   "metadata": {},
   "source": [
    "### Clean Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df76f1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = json_normalize(train_json)\n",
    "test = json_normalize(test_json)\n",
    "\n",
    "train[\"request_title\"] = train[\"request_title\"].str.lower()\n",
    "test[\"request_title\"] = test[\"request_title\"].str.lower()\n",
    "\n",
    "train[\"request_title\"] = train[\"request_title\"].str.replace('\\[request\\]','')\n",
    "test[\"request_title\"] = test[\"request_title\"].str.replace('\\[request\\]','')\n",
    "train[\"request_title\"] = train[\"request_title\"].str.replace('request','')\n",
    "test[\"request_title\"] = test[\"request_title\"].str.replace('request','')\n",
    "train['request_text'] = train['request_text'].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8eb405",
   "metadata": {},
   "source": [
    "### Divide data into training and dev sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03b3ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train[:2800]\n",
    "train_labels = train[:2800]['requester_received_pizza']\n",
    "test_data = test[:]\n",
    "\n",
    "dev_data = train[2800:]\n",
    "dev_labels = train[2800:]['requester_received_pizza']\n",
    "\n",
    "train_data_title = train_data['request_title']\n",
    "dev_data_title = dev_data['request_title']\n",
    "test_data_title = test_data['request_title']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e1eae1",
   "metadata": {},
   "source": [
    "### Inspect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e9510b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('training data shape:', train_data.shape)\n",
    "print('training label shape:', train_labels.shape)\n",
    "print('dev data shape:',     dev_data.shape)\n",
    "print('dev label shape:',      dev_labels.shape)\n",
    "\n",
    "print('training data shape title:',      train_data_title.shape)\n",
    "print('dev data shape title:',      dev_data_title.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bd1ff3",
   "metadata": {},
   "source": [
    "Our task is to detect which posts result in pizza and which do not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e6d6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b81b0b",
   "metadata": {},
   "source": [
    "### Examine Data\n",
    "\n",
    " 1. For first 5 training examples, print the title of request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db366aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_request(num_examples=5):\n",
    "\n",
    "    for i in range(num_examples):\n",
    "        print(train_data.iloc[i]['request_title']) \n",
    "        print(train_data.iloc[i]['request_text'])\n",
    "        print('Received Pizza: ', train_data.iloc[i]['requester_received_pizza']) \n",
    "        print('\\n')  \n",
    "\n",
    "        \n",
    "display_request(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4effe3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Baseline accuracy\n",
    "train_data.groupby('requester_received_pizza').size().plot(kind = \"bar\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6269596",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "train[\"tokens\"] = train[\"request_title\"].apply(tokenizer.tokenize)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f163cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "all_words = [word for tokens in train[\"tokens\"] for word in tokens]\n",
    "sentence_lengths = [len(tokens) for tokens in train[\"tokens\"]]\n",
    "VOCAB = sorted(list(set(all_words)))\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(all_words), len(VOCAB)))\n",
    "print(\"Max sentence length is %s\" % max(sentence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb5f0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fig = plt.figure(figsize=(10, 10)) \n",
    "plt.xlabel('Title length')\n",
    "plt.ylabel('Number of sentences')\n",
    "plt.hist(sentence_lengths)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec315ba",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0c481e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vector = vectorizer.fit_transform(train_data_title)\n",
    "vocab_train = vectorizer.vocabulary_\n",
    "\n",
    "print('Size of vocabulary: ', vector.shape[1])\n",
    "print(\"0th feature: \", vectorizer.get_feature_names_out()[0])\n",
    "print(\"Last feature: \", vectorizer.get_feature_names_out()[-1])\n",
    "print(\"Average number of non-zero features per example: \", round(np.average([row.nnz for row in vector]),3))\n",
    "sparsity = round((vector.nnz / (vector.shape[0] * vector.shape[1])),3)\n",
    "print(f'Fraction of the non-zero entries in the matrix - Sparsity: {sparsity}')\n",
    "\n",
    "vectorizer_dev = CountVectorizer()\n",
    "devvector = vectorizer_dev.fit_transform(dev_data)\n",
    "vocab_dev = vectorizer_dev.vocabulary_\n",
    "dev_missing_words = set(vocab_train.keys()) - set(vocab_dev.keys())\n",
    "print('Dev vocab missing from the training vocab size: ', len(dev_missing_words))\n",
    "print('Fraction of words in dev vocab missing from the training vocab: ', round(len(dev_missing_words)/len(vocab_train),3))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f918b86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "\n",
    "def get_metrics(y_test, y_predicted):  \n",
    "    # true positives / (true positives+false positives)\n",
    "    precision = precision_score(y_test, y_predicted, pos_label=None,\n",
    "                                    average='weighted')             \n",
    "    # true positives / (true positives + false negatives)\n",
    "    recall = recall_score(y_test, y_predicted, pos_label=None,\n",
    "                              average='weighted')\n",
    "    \n",
    "    # harmonic mean of precision and recall\n",
    "    f1 = f1_score(y_test, y_predicted, pos_label=None, average='weighted')\n",
    "    \n",
    "    # true positives + true negatives/ total\n",
    "    accuracy = accuracy_score(y_test, y_predicted)\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "def text_preprocessor(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(\"\\W\",' ',text) # replace non-alphanumeric\n",
    "    text = re.sub(\"_\",' ',text) # replace non-alphanumeric\n",
    "    text = re.sub('\\n', '', text)   \n",
    "    return text\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2), stop_words='english', preprocessor=text_preprocessor)\n",
    "train_vector = vectorizer.fit_transform(train_data_title)\n",
    "\n",
    "dev_vector = vectorizer.transform(dev_data_title)\n",
    "\n",
    "#Produce several Naive Bayes models by varying smoothing (alpha), including one with alpha set approximately to optimize f1 score\n",
    "print('\\n***  Naive Bayes model ***') \n",
    "\n",
    "\n",
    "mnb = MultinomialNB(alpha = 0.5)\n",
    "mnb.fit(train_vector, train_labels)\n",
    "\n",
    "#Evaluate performance on the dev set.\n",
    "pred_mnb = mnb.predict(dev_vector)\n",
    "score = metrics.f1_score(dev_labels, pred_mnb, average=\"weighted\")\n",
    "print(f\"a = 0.5, f1 score = {score}\")\n",
    "\n",
    "accuracy, precision, recall, f1 = get_metrics(dev_labels, pred_mnb)\n",
    "print(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cf064f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad9feb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_most_important_features(vectorizer, model, n=5):\n",
    "    index_to_word = {v:k for k,v in vectorizer.vocabulary_.items()}\n",
    "    \n",
    "    # loop for each class\n",
    "    classes ={}\n",
    "    for class_index in range(model.coef_.shape[0]):\n",
    "        word_importances = [(el, index_to_word[i]) for i,el in enumerate(model.coef_[class_index])]\n",
    "        sorted_coeff = sorted(word_importances, key = lambda x : x[0], reverse=True)\n",
    "        tops = sorted(sorted_coeff[:n], key = lambda x : x[0])\n",
    "        bottom = sorted_coeff[-n:]\n",
    "        classes[class_index] = {\n",
    "            'tops':tops,\n",
    "            'bottom':bottom\n",
    "        }\n",
    "    return classes\n",
    "\n",
    "vectorizer = CountVectorizer(ngram_range=(2, 2), stop_words='english', preprocessor=text_preprocessor)\n",
    "train_vector = vectorizer.fit_transform(train_data_title)\n",
    "\n",
    "lr = LogisticRegression(C=.5, solver=\"liblinear\", multi_class=\"auto\")\n",
    "lr.fit(train_vector, train_labels)\n",
    "pred_lr = lr.predict(dev_vector)\n",
    "score = metrics.f1_score(dev_labels, pred_lr, average=\"weighted\")\n",
    "print('\\n***  Logistic Regression model ***') \n",
    "print(f\"a = 0.5, f1 score = {score}\")\n",
    "\n",
    "accuracy, precision, recall, f1 = get_metrics(dev_labels, pred_lr)\n",
    "print(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))        \n",
    "\n",
    "importance = get_most_important_features(vectorizer, lr, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec43d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance[0]['tops']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87de641",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_important_words(top_scores, top_words, bottom_scores, bottom_words, name):\n",
    "    y_pos = np.arange(len(top_words))\n",
    "    top_pairs = [(a,b) for a,b in zip(top_words, top_scores)]\n",
    "    top_pairs = sorted(top_pairs, key=lambda x: x[1])\n",
    "    \n",
    "    bottom_pairs = [(a,b) for a,b in zip(bottom_words, bottom_scores)]\n",
    "    bottom_pairs = sorted(bottom_pairs, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    top_words = [a[0] for a in top_pairs]\n",
    "    top_scores = [a[1] for a in top_pairs]\n",
    "    \n",
    "    bottom_words = [a[0] for a in bottom_pairs]\n",
    "    bottom_scores = [a[1] for a in bottom_pairs]\n",
    "    \n",
    "    fig = plt.figure(figsize=(10, 10))  \n",
    "\n",
    "    plt.subplot(121)\n",
    "    plt.barh(y_pos,bottom_scores, align='center', alpha=0.5)\n",
    "    plt.title('No Pizza', fontsize=20)\n",
    "    plt.yticks(y_pos, bottom_words, fontsize=14)\n",
    "    plt.suptitle('Key words', fontsize=16)\n",
    "    plt.xlabel('Importance', fontsize=20)\n",
    "    \n",
    "    plt.subplot(122)\n",
    "    plt.barh(y_pos,top_scores, align='center', alpha=0.5)\n",
    "    plt.title('Pizza', fontsize=20)\n",
    "    plt.yticks(y_pos, top_words, fontsize=14)\n",
    "    plt.suptitle(name, fontsize=16)\n",
    "    plt.xlabel('Importance', fontsize=20)\n",
    "    \n",
    "    plt.subplots_adjust(wspace=0.8)\n",
    "    plt.show()\n",
    "\n",
    "top_scores = [a[0] for a in importance[0]['tops']]\n",
    "top_words = [a[1] for a in importance[0]['tops']]\n",
    "bottom_scores = [a[0] for a in importance[0]['bottom']]\n",
    "bottom_words = [a[1] for a in importance[0]['bottom']]\n",
    "\n",
    "plot_important_words(top_scores, top_words, bottom_scores, bottom_words, \"Most important words for relevance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887d309a",
   "metadata": {},
   "outputs": [],
   "source": [
    " #Tfidf Vectorizer\n",
    "tfidfvectorizer = TfidfVectorizer()\n",
    "train_vector_tfIdf = tfidfvectorizer.fit_transform(train_data_title)\n",
    "\n",
    " #transform dev tfidfvectorizer\n",
    "dev_vector_tfIdf = tfidfvectorizer.transform(dev_data_title) \n",
    "\n",
    "#default is penalty=\"l2\"\n",
    "lr_tfIdf =  LogisticRegression(C=100, solver=\"liblinear\", multi_class=\"auto\")\n",
    "lr_tfIdf.fit(train_vector_tfIdf, train_labels)   \n",
    "\n",
    "pred_tfIdf = lr_tfIdf.predict(dev_vector_tfIdf)\n",
    "\n",
    "score = metrics.f1_score(dev_labels, pred_tfIdf, average=\"weighted\")\n",
    "print(f\"\\n******** Tfidf Vectorizer ********\")\n",
    "print(f\"\\nTfidf C = 100, f1 score = {score}, vocab size =\", len(tfidfvectorizer.vocabulary_)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d227a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understand nature of the data .info() .describe()\n",
    "# Histograms and boxplots \n",
    "# Value counts \n",
    "# Missing data \n",
    "# Correlation between the metrics \n",
    "# Explore interesting themes \n",
    "    # Wealthy survive? \n",
    "    # By location \n",
    "    # Age scatterplot with ticket price \n",
    "    # Young and wealthy Variable? \n",
    "    # Total spent? \n",
    "# Feature engineering \n",
    "# preprocess data together or use a transformer? \n",
    "    # use label for train and test   \n",
    "# Scaling?\n",
    "\n",
    "# Model Baseline \n",
    "# Model comparison with CV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4eb983",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(no_pizza[\"requester_account_age_in_days_at_request\"].mean())\n",
    "print(pizza[\"requester_account_age_in_days_at_request\"].mean())\n",
    "print('\\nrequest_number_of_comments_at_retrieval')\n",
    "print(no_pizza[\"request_number_of_comments_at_retrieval\"].mean())\n",
    "print(pizza[\"request_number_of_comments_at_retrieval\"].mean())\n",
    "print('\\nnumber_of_upvotes_of_request_at_retrieval')\n",
    "print(no_pizza[\"number_of_upvotes_of_request_at_retrieval\"].mean())\n",
    "print(pizza[\"number_of_upvotes_of_request_at_retrieval\"].mean())\n",
    "print('\\nrequester_number_of_comments_in_raop_at_request')\n",
    "print(no_pizza[\"requester_number_of_comments_in_raop_at_request\"].mean())\n",
    "print(pizza[\"requester_number_of_comments_in_raop_at_request\"].mean())\n",
    "print('\\nrequester_number_of_posts_at_retrieval')\n",
    "print(no_pizza[\"requester_number_of_posts_at_retrieval\"].mean())\n",
    "print(pizza[\"requester_number_of_posts_at_retrieval\"].mean())\n",
    "print('\\nrequester_number_of_subreddits_at_request')\n",
    "print(no_pizza[\"requester_number_of_subreddits_at_request\"].mean())\n",
    "print(pizza[\"requester_number_of_subreddits_at_request\"].mean())\n",
    "\n",
    "no_pizza[\"requester_user_flair\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8143c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(pizza[\"request_title\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538f068d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
